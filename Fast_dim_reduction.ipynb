{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Fast_dim_reduction.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO6oxeDwMi9Y4khQaLDBchT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LeyloJohnny/Computational-Neuroscience/blob/main/Fast_dim_reduction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ojghLrcJJt7o"
      },
      "source": [
        "#get data from google drive\n",
        "!gdown --id 1LvD4yrrrNhc98udFEl59Aa3Z7w_xoZ6S\n",
        "#unzip data\n",
        "!unzip 'Leyla_Data.zip'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v_0E8w8bJne9"
      },
      "source": [
        "#imports\n",
        "import pickle\n",
        "import numpy as np\n",
        "#imports required for pca\n",
        "import pandas as pd\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "from sklearn.manifold import TSNE\n",
        "from operator import add\n",
        "from operator import sub\n",
        "\n",
        "#define variables\n",
        "\n",
        "time_ranges = [1000, 1500, 1700]\n",
        "starting_time = 0\n",
        "neuron_ID = 0\n",
        "nNeurons = 20\n",
        "window = 100\n",
        "t_range = 0\n",
        "colors = ['b', 'c', 'g']\n",
        "\n",
        "#access data\n",
        "\n",
        "#create a list of model name strings\n",
        "models = ['BN.pkl', \"SFA.pkl\", \"Cluster.pkl\"]\n",
        "\n",
        "#create a list of model names\n",
        "model_names = [\"Balanced Network\", \"Clustered with SFA\", \"Clustered wo SFA\"]\n",
        "\n",
        "#create lists to store model data\n",
        "models_x = []\n",
        "models_y = []\n",
        "\n",
        "#for all models\n",
        "for i in range(len(models)):\n",
        "\n",
        "  #open pickle file\n",
        "  with open(models[i], 'rb') as f:\n",
        "    #get x (actual data: time * neuron id)\n",
        "    x = pickle.load(f)\n",
        "    #get y (information on data, such as inhibitory/excitatory neurons, clusters, etc.)\n",
        "    y = pickle.load(f)\n",
        "    #add x and y to the models lists\n",
        "    models_x.append(x)\n",
        "    models_y.append(y)\n",
        "\n",
        "for t_range in range(len(time_ranges)):\n",
        "\n",
        "    time_range_models = []\n",
        "\n",
        "    time_range = time_ranges[t_range]\n",
        "    starting_time = starting_times[s_time]\n",
        "\n",
        "    for model in range(len(models)):\n",
        "      #set condition: returns True for time <= 5 s\n",
        "      condition = [True if i >starting_time and i <= time_range + starting_time else False for i in models_x[model][0]]\n",
        "\n",
        "      #extract new x and y values for array based on condition\n",
        "      new_x = np.extract(condition, models_x[model][0])\n",
        "      new_y = np.extract(condition, models_x[model][1])\n",
        "\n",
        "      new = [new_x, new_y]\n",
        "\n",
        "      time_range_models.append(new)\n",
        "\n",
        "    one_cluster = []\n",
        "\n",
        "    for model in range(len(models)):\n",
        "\n",
        "      condition = [True if i > neuron_ID and i <= neuron_ID+nNeurons else False for i in time_range_models[model][1]]\n",
        "\n",
        "      #extract new x and y values for array based on condition\n",
        "      new_x = np.extract(condition, time_range_models[model][0])\n",
        "      new_y = np.extract(condition, time_range_models[model][1])\n",
        "\n",
        "      new = [new_x, new_y]\n",
        "\n",
        "      one_cluster.append(new)\n",
        "\n",
        "    neuron_array = np.arange(neuron_ID, neuron_ID+nNeurons, 1)\n",
        "    \n",
        "    #define a time window (in milliseconds)\n",
        "\n",
        "    neurons = nNeurons\n",
        "\n",
        "    file_name = \"spike_count_matrix_tr_\" + str(time_range) + \"_tw_\" + str(window)\n",
        "\n",
        "    #calculate number of intervals based on recorded time span / time window\n",
        "    intervals = int(time_range/window)\n",
        "\n",
        "    #define which neurons to focus on (here: only excitatory neurons 0 - 4000)\n",
        "    #neurons = int(np.max(one_cluster[i][1])+1) \n",
        "\n",
        "    #how many intervals do we have?\n",
        "    print(\"We have \" + str(intervals) + \" intervals, each \" + str(window) + \"ms long. \\n\")\n",
        "\n",
        "    spike_count_matrix = np.zeros((len(models), intervals, neurons))\n",
        "\n",
        "    #for every model\n",
        "    for i in range(len(models)):\n",
        "      #get the data of the current model\n",
        "      current_model_x = np.array(one_cluster[i])\n",
        "\n",
        "      #create a zeros array with size time * neurons (5000*4000)\n",
        "      spike_counts = np.zeros((intervals, neurons))\n",
        "\n",
        "      #for every interval\n",
        "      for j in range(intervals):\n",
        "\n",
        "        #get the indices of time points within the current interval\n",
        "        interval_time_points = np.where(np.logical_and(window*j + starting_time <= current_model_x[0], current_model_x[0] < window*(j+1)+starting_time))\n",
        "\n",
        "        #get data indexed by current interval\n",
        "        interval_data = current_model_x[1][interval_time_points]\n",
        "\n",
        "        #for every neuron\n",
        "        for k in range(neurons):\n",
        "\n",
        "            #get the indices of the current neuron within the defined time interval\n",
        "            current_neuron = np.where(interval_data == neuron_array[k])\n",
        "\n",
        "            #get the number of spikes based on the amount of neuron indices within the defined time interval\n",
        "            spikes = current_neuron[0].shape[0]\n",
        "\n",
        "            #add spikes number to spike count matrix\n",
        "            spike_counts[j, k] = spikes\n",
        "\n",
        "      #add spike count matrix for every model\n",
        "      spike_count_matrix[i] = spike_counts\n",
        "\n",
        "    np.save(file_name, spike_count_matrix)\n",
        "\n",
        "    #print data\n",
        "    print(\"The spike count matrix has the shape: \" + str(spike_count_matrix.shape) + \" representing \" + \n",
        "          str(spike_count_matrix.shape[0]) + \" models, \" + str(spike_count_matrix.shape[1]) + \" intervals, and \" + str(spike_count_matrix.shape[2]) + \" neurons. \\n\")\n",
        "    \n",
        "    #define titles for columns and rows in the dataframe that will be generated from the spike count matrix\n",
        "    cols = ['Neuron{}'.format(int(col+1)) for col in range(neurons)] #rows represent the neurons (= features)\n",
        "    rows = ['Interval{}'.format(int(row+1)) for row in range(intervals)] #columns represent the time intervals/time bins (= samples)\n",
        "\n",
        "    #create list for the data to be analyzed for each model\n",
        "    models_data = []\n",
        "\n",
        "    #for every model'\n",
        "    for model in range(len(models)):\n",
        "      #create a dataframe from the spike count matrix\n",
        "      current = spike_count_matrix[model]\n",
        "      data = pd.DataFrame(data = current, index = rows, columns = cols)\n",
        "      #add the dataframe to the list (containing one dataframe per model)\n",
        "      models_data.append(data)\n",
        "\n",
        "    silent_neurons = []\n",
        "    all_models_silent = []\n",
        "\n",
        "    for model in range(len(models)):\n",
        "      model_silent_neurons = []\n",
        "      for neuron in range(neurons):\n",
        "        if models_data[model].iloc[:, neuron].sum(0) == 0:\n",
        "          model_silent_neurons.append(neuron+1)\n",
        "      silent_neurons.append(model_silent_neurons)\n",
        "    \n",
        "    for model in range(len(models)):\n",
        "      for neuron_number in range(len(silent_neurons[model])):\n",
        "        neuron = silent_neurons[model][neuron_number]\n",
        "        del models_data[model][\"Neuron\" + str(neuron)]\n",
        "        \n",
        "    #create list for the pca results for each model\n",
        "    pca_results = []\n",
        "    #create list for the eigenvalues for each model\n",
        "    eigenvalues = []\n",
        "    #create list for the variance ratio for each model\n",
        "    variance_ratio = []\n",
        "    #create list for the number of principal components for each model (using a certain variance)\n",
        "    n_components = []\n",
        "    #create list for the cumulative variance\n",
        "    cumulative_variance = []\n",
        "\n",
        "    #initiate pca\n",
        "    pca = PCA()\n",
        "\n",
        "    #for every model\n",
        "    for i in range(len(models)): \n",
        "      #standardize data: transform the data onto unit scale (mean = 0 and variance = 1)\n",
        "      standardized = StandardScaler().fit_transform(models_data[i])\n",
        "      #fit the data\n",
        "      pca.fit(standardized)\n",
        "\n",
        "      #add eigenvalues to eigenvalue list\n",
        "      eigenvalues.append(pca.explained_variance_)\n",
        "      #add explained variance ratio to variance ratio list\n",
        "      variance_ratio.append(pca.explained_variance_ratio_)\n",
        "\n",
        "      #get the pca results\n",
        "      final_data = pca.transform(models_data[i])\n",
        "      #add the resulting data to the list of results for each model\n",
        "      pca_results.append(final_data)\n",
        "\n",
        "      #calculate cumulative variance:\n",
        "\n",
        "      #set cumulative to zero for every new model\n",
        "      cumulative = 0\n",
        "      #create empty array for cumulative variance of the current model\n",
        "      current_cumulative = np.zeros(len(variance_ratio[i]))\n",
        "      #for every principal component (all variance values)\n",
        "      for variance in range(len(variance_ratio[i])):\n",
        "        #add the current components variance to the cumulative value\n",
        "        cumulative += variance_ratio[i][variance]\n",
        "        #set the current models cumulative value to the current cumulative\n",
        "        current_cumulative[variance] = cumulative\n",
        "      #append current models cumulative to list of all models cumulative variance\n",
        "      cumulative_variance.append(current_cumulative)\n",
        "\n",
        "      #calculate the amount of principal components describing a defined amount of explained variance (e.g. 90%)\n",
        "      current_components = np.where(cumulative_variance[i] < 0.9)\n",
        "      n_components.append(int(current_components[0].shape[0]) + 1)\n",
        "    \n",
        "    dim_reduction_number = []\n",
        "\n",
        "    for model in range(len(models)): \n",
        "      #how many principal components do we have?\n",
        "      print(\"Model \" + str(model) + \" has \" + str(n_components[model]) + \" principal components.\")\n",
        "      #set the current value to zero\n",
        "      current = 0\n",
        "\n",
        "      #for every principal component\n",
        "      for j in range(n_components[model]):\n",
        "        #(eigenvalue/total amount of variance explained)²\n",
        "        squared = (eigenvalues[model][j]/sum(eigenvalues[model][:]))**2\n",
        "        #add the result to the current value\n",
        "        current += squared\n",
        "\n",
        "      #divide 1 by the sum of all results\n",
        "      result = 1/current\n",
        "\n",
        "      dim_reduction_number.append(str(int(result)))\n",
        "\n",
        "      #print the results\n",
        "      print(\"Model \" + str(model) + \" has \" + str(int(result)) + \" dimensions based on dimensionality measure used by Mazzucato et al.\")\n",
        "    \n",
        "    #create list for all dimensionality values for every model\n",
        "    all_dimensionality = []\n",
        "\n",
        "    #for every model\n",
        "    for model in range(len(models)): \n",
        "        #access the eigenvalues of the current model\n",
        "        current_model = all_eigenvalues[model]\n",
        "        #create list for the dimensionality values of each sample\n",
        "        samples = []\n",
        "\n",
        "        #for every sample\n",
        "        for sample in range(ensemble_size):\n",
        "          #access the eigenvalues of the current sample\n",
        "          current_sample = current_model[sample]\n",
        "          #create a list for the dimensionality values of the current sample's repeats\n",
        "          all_repeats = []\n",
        "\n",
        "          #for every repeat\n",
        "          for repeat in range(repeats):\n",
        "            #set the current value to zero\n",
        "            current = 0\n",
        "            #calculate the total variance of all neuron's eigenvalues within the current sample and repeat\n",
        "            total_variance = sum(current_sample[repeat][:])\n",
        "\n",
        "            #if the total variance is not zero, calculate dimensionality:\n",
        "            if total_variance != 0:\n",
        "              #for every neurons within the current sample and repeat\n",
        "              for neuron in range(len(current_sample[repeat])):\n",
        "                #squared = (eigenvalue/total amount of variance explained)²\n",
        "                squared = (current_sample[repeat][neuron]/total_variance)**2  \n",
        "                #add the result to the current value\n",
        "                current += squared\n",
        "\n",
        "              #calculate dimensionality: 1 divided by current value\n",
        "              dimensionality = 1/current\n",
        "\n",
        "            #else (if the total variance is zero)\n",
        "            else:\n",
        "              print(\"weird...\")\n",
        "              #set the result to zero to avoid division by zero\n",
        "              dimensionality = 0\n",
        "\n",
        "            #add the current dimensionality to the list of all repeats  \n",
        "            all_repeats.append(dimensionality)\n",
        "          #calculate mean and standard deviation for all repeats of the current sample\n",
        "          mean = np.mean(all_repeats[:])\n",
        "          std = np.std(all_repeats[:])\n",
        "          #add means and stds as a tuple to a list for all samples\n",
        "          samples.append((mean, std))\n",
        "        #add all dimensionality values from all samples to the list of all dimensionalities\n",
        "        all_dimensionality.append(samples)\n",
        "        \n",
        "\n",
        "    #define figure fig with a certain size\n",
        "    fig = plt.figure(figsize = (10,6))\n",
        "    #define axis, two axis, index position = 1\n",
        "    ax = fig.add_subplot(1,1,1) \n",
        "    #set labels and title of figure\n",
        "    ax.set_xlabel('Ensemble size', fontsize = 15)\n",
        "    ax.set_ylabel('Dimensionality', fontsize = 15)\n",
        "    ax.set_title('Dimensionality reduction summary', fontsize = 20)\n",
        "\n",
        "    #for tuple of target and color: (target, color)\n",
        "    for model in range(len(models)):\n",
        "        #get lists of all means and standard deviations\n",
        "        all_means = [x[0] for x in all_dimensionality[model]]\n",
        "        all_std = [x[1] for x in all_dimensionality[model]]\n",
        "\n",
        "        #calculate positive and negative error\n",
        "        pos_err = list(map(add, all_means, all_std)) \n",
        "        neg_err = list(map(sub, all_means, all_std))\n",
        "\n",
        "        #plot the data\n",
        "        ax.plot(all_steps[model], all_means, color = colors[model], label = model_names[model])\n",
        "        #fill area of positive and negative errors\n",
        "        ax.fill_between(all_steps[model], pos_err, neg_err, facecolor = colors[model], alpha=0.3)\n",
        "        plt.legend(loc = \"lower right\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}