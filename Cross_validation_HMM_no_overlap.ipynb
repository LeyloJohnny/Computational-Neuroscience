{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Cross_validation_HMM_no_overlap.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNf8hzytYQh+kVIShnxK3LU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LeyloJohnny/Computational-Neuroscience/blob/main/Cross_validation_HMM_no_overlap.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fXp0AaghDjBt",
        "outputId": "478d6f13-fd87-47ce-9560-af5c9e9d62bf"
      },
      "source": [
        "#get data from google drive\n",
        "!gdown --id 1qblO7uQCJ6KKclqS-rhvXfQ7ccWcmxaA\n",
        "#unzip data\n",
        "!unzip 'NewData.zip'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1qblO7uQCJ6KKclqS-rhvXfQ7ccWcmxaA\n",
            "To: /content/NewData.zip\n",
            "127MB [00:01, 123MB/s]\n",
            "Archive:  NewData.zip\n",
            "  inflating: Cluster.pkl             \n",
            "  inflating: BN.pkl                  \n",
            "  inflating: SFA.pkl                 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aic0fZFwD_tA",
        "outputId": "4af10153-6c76-4c0c-b195-4bfaf175c40e"
      },
      "source": [
        "!unzip 'NewData.zip'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  NewData.zip\n",
            "replace Cluster.pkl? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dnv-1eOTEFG_"
      },
      "source": [
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "#create a list of model name strings\n",
        "#models = ['BN.pkl', \"SFA.pkl\", \"Cluster.pkl\"]\n",
        "\n",
        "#create a list of model names\n",
        "#model_names = [\"Balanced Network\", \"Clustered with SFA\", \"Clustered wo SFA\"]\n",
        "\n",
        "#create a list of model name strings\n",
        "models = ['new_BN.pkl', \"new_SFA.pkl\", \"new_Cluster.pkl\"]\n",
        "\n",
        "#modes = [\"E\", \"I_and_E\"]\n",
        "\n",
        "#create a list of model names\n",
        "model_names = [\"Balanced Network\", \"Clustered with SFA\", \"Clustered wo SFA\"]\n",
        "\n",
        "#create lists to store model data\n",
        "models_x = []\n",
        "models_y = []\n",
        "\n",
        "#for all models\n",
        "for i in range(len(models)):\n",
        "\n",
        "  #open pickle file\n",
        "  with open(models[i], 'rb') as f:\n",
        "    #get x (actual data: time * neuron id)\n",
        "    x = pickle.load(f)\n",
        "    #get y (information on data, such as inhibitory/excitatory neurons, clusters, etc.)\n",
        "    y = pickle.load(f)\n",
        "    #add x and y to the models lists\n",
        "    models_x.append(x)\n",
        "    models_y.append(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jTLXSYJXEInD"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "time_range_models = []\n",
        "\n",
        "time_range = 10000\n",
        "starting_time = 0\n",
        "\n",
        "for model in range(len(models)):\n",
        "  #set condition: returns True for time <= 5 s\n",
        "  condition = [True if i > starting_time and i <= time_range + starting_time else False for i in models_x[model][0]]\n",
        "\n",
        "  #extract new x and y values for array based on condition\n",
        "  new_x = np.extract(condition, models_x[model][0])\n",
        "  new_y = np.extract(condition, models_x[model][1])\n",
        "\n",
        "  new = [new_x, new_y]\n",
        "\n",
        "  time_range_models.append(new)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NaJ5YbmoENgq"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "one_cluster = []\n",
        "\n",
        "neuron_ID = 0\n",
        "\n",
        "for model in range(len(models)):\n",
        "\n",
        "  condition = [True if i > neuron_ID and i <= neuron_ID+4000 else False for i in time_range_models[model][1]]\n",
        "\n",
        "  #extract new x and y values for array based on condition\n",
        "  new_x = np.extract(condition, time_range_models[model][0])\n",
        "  new_y = np.extract(condition, time_range_models[model][1])\n",
        "\n",
        "  #create figure\n",
        "  figure = plt.figure(dpi=100)\n",
        "  #create subplot\n",
        "  ax = figure.add_subplot(1,1,1)\n",
        "  #plot both dimensions of x (time, neuron id)\n",
        "  plt.plot(new_x,new_y,'.k',markersize = 0.8) \n",
        "\n",
        "  #label the plot\n",
        "  ax.set_xlabel('Time in ms', fontsize = 10)\n",
        "  ax.set_ylabel('Neuron ID', fontsize = 10)\n",
        "  ax.set_title(model_names[model], fontsize = 15)\n",
        "\n",
        "  new = [new_x, new_y]\n",
        "\n",
        "  one_cluster.append(new)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QdeZPvZVEdH3"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#define a time window (in milliseconds)\n",
        "window = 100\n",
        "\n",
        "file_name = \"spike_count_matrix_tr_\" + str(time_range) + \"_tw_\" + str(window)\n",
        "\n",
        "#calculate number of intervals based on recorded time span / time window\n",
        "intervals = int(np.max(time_range_models[0][0]+ 1-starting_time)/window)\n",
        "\n",
        "#define which neurons to focus on (here: only excitatory neurons 0 - 4000)\n",
        "Nneurons = int(np.max(time_range_models[0][1])+1) -1000\n",
        "\n",
        "#how many intervals do we have?\n",
        "print(\"We have \" + str(intervals) + \" intervals, each \" + str(window) + \"ms long. \\n\")\n",
        "\n",
        "spike_count_matrix_average = np.zeros((len(models), intervals, Nneurons))\n",
        "\n",
        "#for every model\n",
        "for i in range(len(models)):\n",
        "  #get the data of the current model\n",
        "  current_model_x = np.array(time_range_models[i])\n",
        "\n",
        "  #create a zeros array with size time * neurons (5000*4000)\n",
        "  spike_counts = np.zeros((intervals, Nneurons))\n",
        "\n",
        "  #for every interval\n",
        "  for j in range(intervals):\n",
        "\n",
        "    #get the indices of time points within the current interval\n",
        "    interval_time_points = np.where(np.logical_and(window*j + starting_time <= current_model_x[0], current_model_x[0] < window*(j+1) + starting_time))\n",
        "\n",
        "    #get data indexed by current interval\n",
        "    interval_data = current_model_x[1][interval_time_points]\n",
        "    \n",
        "    #for every neuron\n",
        "    for k in range(Nneurons):\n",
        "\n",
        "        #get the indices of the current neuron within the defined time interval\n",
        "        current_neuron = np.where(interval_data == k)\n",
        "\n",
        "        #get the number of spikes based on the amount of neuron indices within the defined time interval\n",
        "        spikes = current_neuron[0].shape[0]\n",
        "\n",
        "        #add spikes number to spike count matrix\n",
        "        spike_counts[j, k] = spikes\n",
        "  \n",
        "  #add spike count matrix for every model\n",
        "  spike_count_matrix_average[i] = spike_counts\n",
        "\n",
        "#print data\n",
        "print(\"The spike count matrix has the shape: \" + str(spike_count_matrix_average.shape) + \" representing \" + \n",
        "      str(spike_count_matrix_average.shape[0]) + \" models, \" + str(spike_count_matrix_average.shape[1]) + \" intervals, and \" + str(spike_count_matrix_average.shape[2]) + \" neurons. \\n\")\n",
        "print(\"Spike count matrix: \\n\")\n",
        "print(spike_count_matrix_average)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9TahJ-sUElwI"
      },
      "source": [
        "#define titles for columns and rows in the dataframe that will be generated from the spike count matrix\n",
        "cols = ['Neuron{}'.format(int(col+1)) for col in range(Nneurons)] #rows represent the neurons (= features)\n",
        "rows = ['Interval{}'.format(int(row+1)) for row in range(intervals)] #columns represent the time intervals/time bins (= samples)\n",
        "\n",
        "#create list for the data to be analyzed for each model\n",
        "models_data_average = []\n",
        "\n",
        "#for every model\n",
        "for i in range(len(models)):\n",
        "  #create a dataframe from the spike count matrix\n",
        "  current = spike_count_matrix_average[i]\n",
        "  data_average = pd.DataFrame(data = current, index = rows, columns = cols)\n",
        "  #add the dataframe to the list (containing one dataframe per model)\n",
        "  models_data_average.append(data_average)\n",
        "\n",
        "#exemplarily show the dataframe for the second model\n",
        "models_data_average[1].head(intervals)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IIe7DYu6Eoi9"
      },
      "source": [
        "model_clusters = []\n",
        "\n",
        "cluster_length = 80\n",
        "clusters = int(Nneurons/cluster_length)\n",
        "\n",
        "for model in range(len(models)):\n",
        "\n",
        "  all_cluster_df = pd.DataFrame()\n",
        "\n",
        "  for cluster in range(clusters):\n",
        "    cols = range(cluster*cluster_length, cluster*cluster_length + cluster_length)\n",
        "    series = models_data_average[model][models_data_average[1].columns[cols]].mean(axis = 1)\n",
        "    all_cluster_df.insert(cluster,\"cluster\" + str(cluster+1),series,True)\n",
        "  \n",
        "  model_clusters.append(all_cluster_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wdGnt23IEq-M"
      },
      "source": [
        "all_cluster_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rwR09OqcEs0E"
      },
      "source": [
        " data_array = []\n",
        " for model in range(len(models)): \n",
        "  new_array = model_clusters[model].to_numpy()\n",
        "  data_array.append(new_array)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m3KiZtTmEwHr"
      },
      "source": [
        "!pip install git+git://github.com/lindermanlab/ssm\n",
        "import ssm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2tFUeebbEzGx"
      },
      "source": [
        "import pickle\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "import scipy.stats as st\n",
        "import multiprocessing\n",
        "\n",
        "import autograd.numpy as np\n",
        "import autograd.numpy.random as npr\n",
        "npr.seed(3)\n",
        "\n",
        "import ssm\n",
        "from ssm.util import find_permutation\n",
        "from ssm.plots import gradient_cmap, white_to_color_cmap\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import seaborn as sns\n",
        "sns.set_style(\"white\")\n",
        "sns.set_context(\"talk\")\n",
        "\n",
        "color_names=['windows blue','red','amber','faded green','dusty purple','orange','steel blue','pink',\n",
        "             'greyish','mint','clay','light cyan','forest green','pastel purple','salmon','dark brown',\n",
        "             'lavender','pale green','dark red','gold','dark teal','rust','fuchsia','pale orange',\n",
        "             'cobalt blue','mahogany','cloudy blue','dark pastel green','dust','electric lime','fresh green','light eggplant']\n",
        "\n",
        "color = sns.xkcd_palette(color_names)\n",
        "cmap = gradient_cmap(color)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RX8tBeTWE1jG"
      },
      "source": [
        "obs_dim = 50\n",
        "\n",
        "#how many subsamples?\n",
        "parts = 10\n",
        "\n",
        "#how long is each subsample\n",
        "part_length = int(model_clusters[1].shape[0]/parts)\n",
        "time_bins = part_length\n",
        "print(part_length)\n",
        "\n",
        "#create list for subsamples\n",
        "models_subsamples = []\n",
        "\n",
        "#for all models\n",
        "for model in range(len(models)):\n",
        "  #create lists for current part data\n",
        "  current_part_data = []\n",
        "  models_parts = []\n",
        "\n",
        "  #for every sample (sample: part of the whole time range of the data)\n",
        "  for part in range(parts): \n",
        "    #define part time range\n",
        "    part_start = part*part_length \n",
        "    part_end = (part+1)*part_length\n",
        "\n",
        "    #extract current part data based on defined time range\n",
        "    current_part = model_clusters[model][part_start:part_end]\n",
        "    current_part_data.append(current_part.to_numpy())\n",
        "\n",
        "    #sample = current_part.sample(int(obs_dim), axis = 'columns')\n",
        "\n",
        "    #for neuron in range(sample.shape[1]):\n",
        "      #del current_part_data[part][list(sample)[neuron]]\n",
        "    \n",
        "    #models_parts.append(sample.to_numpy())\n",
        "\n",
        "  #models_subsamples.append(models_parts)\n",
        "  models_subsamples.append(current_part_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VHkjJbF4E6hD"
      },
      "source": [
        "from sklearn.model_selection import KFold\n",
        "\n",
        "Ks = np.arange(2,30,2)\n",
        "\n",
        "X = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
        "\n",
        "synthetic_data = models_subsamples[2]\n",
        "\n",
        "subsamples = len(synthetic_data)\n",
        "\n",
        "runs = 5\n",
        "\n",
        "#10 different number of latent states\n",
        "#5-fold cross-validation\n",
        "#That's 50 model fits! Thankfully this can be parallelized fairly easily\n",
        "#max_states = 20\n",
        "N_iters = 5\n",
        "ll_training = np.zeros((len(Ks),runs))\n",
        "ll_heldout = np.zeros((len(Ks),runs))\n",
        "\n",
        "nTrain = int(0.8 * len(synthetic_data))\n",
        "nTest = int(0.2 * len(synthetic_data))\n",
        "\n",
        "hmm_z_ms = np.zeros((len(Ks),time_bins))\n",
        "\n",
        "#Outer loop over the parameter for which you're doing model selection for\n",
        "for iS, num_states in enumerate(Ks):\n",
        "\n",
        "    print(\"----------------------------\" + str(iS) + \" ---------------------------\")\n",
        "    #iK = 0\n",
        "    kf = KFold(n_splits=runs, shuffle = True) \n",
        "\n",
        "    for iK, (train_index, test_index) in enumerate(kf.split(X)):\n",
        "\n",
        "        print(\"iK: \" + str(iK) + \" TRAIN:\", train_index, \"TEST:\", test_index)\n",
        "          \n",
        "        #Split data\n",
        "        training_data = [synthetic_data[ii] for ii in train_index]\n",
        "        test_data = [synthetic_data[ii] for ii in test_index]\n",
        "\n",
        "        #Create HMM object to fit\n",
        "        hmm = ssm.HMM(num_states, obs_dim, observations=\"gaussian\")\n",
        "\n",
        "        #fit on training data\n",
        "        hmm_lls = hmm.fit(training_data, method=\"em\", num_iters=N_iters)#, init_method=\"kmeans\")\n",
        "          \n",
        "        #Compute log-likelihood for each dataset\n",
        "        ll_training[iS,iK] = hmm.log_probability(training_data)/nTrain\n",
        "        ll_heldout[iS,iK] = hmm.log_probability(test_data)/nTest\n",
        "\n",
        "        #iK += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XgT4RY9qE9j5"
      },
      "source": [
        "#Plot the log-likelihood of the training and test datasets\n",
        "\n",
        "fig, axes = plt.subplots(1,2,figsize=(20,7),sharey=False)\n",
        "\n",
        "#Training data\n",
        "axes[0].plot(Ks, ll_training,'.')\n",
        "axes[0].plot(Ks, np.mean(ll_training,axis=1),'-ok')\n",
        "axes[0].set_title('Training Log-likelihood')\n",
        "axes[0].set_ylabel('Log-likelihood / data-point')\n",
        "\n",
        "axes[1].plot(Ks, ll_heldout,'.')\n",
        "axes[1].plot(Ks, np.mean(ll_heldout,axis=1),'-ok')\n",
        "axes[1].set_title('Test Log-likelihood')\n",
        "\n",
        "mean_test_data = np.mean(ll_heldout,axis=1)\n",
        "\n",
        "n_max = max(mean_test_data)\n",
        "n_max_index = Ks[np.where(mean_test_data == n_max)]\n",
        "\n",
        "for ax in axes:\n",
        "    ax.set_xticks(Ks)\n",
        "    ax.set_xlabel('Number of States')\n",
        "    #ax.vlines(n_max_index,*ax.get_ylim(),ls='--',color='k')\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}